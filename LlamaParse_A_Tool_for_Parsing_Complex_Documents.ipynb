{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ranzhang/Documentation/blob/master/LlamaParse_A_Tool_for_Parsing_Complex_Documents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LlamaParse - Parsing Complex Documents\n",
        "\n",
        "With the release of [LlamaParse](https://github.com/run-llama/llama_parse) and [LlamaCloud](https://cloud.llamaindex.ai), LlamaIndex is demonstrating the next step of evolution for its offerings!\n",
        "\n",
        "From the repository:\n",
        "\n",
        "> LlamaParse is an API created by LlamaIndex to efficiently parse and represent files for efficient retrieval and context augmentation using LlamaIndex frameworks.\n",
        "\n",
        "What LlamaIndex has done is created an API Endpoint that we can access (currently for free up to 10,000 pages of PDFs a day) that will parse out PDF files into either plain-text or markdown. That second one means we have a way to retain structural data that can be leveraged for more structural queries!\n",
        "\n",
        "They've also [recently released](https://www.llamaindex.ai/blog/llamaindex-v0-10-838e735948f8) their v0.10 which, similar to LangChain's v0.1.0, provides some stability and methodological changes to move LlamaIndex into the production-ready space. (seeyah later `ServiceContext`!)\n",
        "\n",
        "Let's dive in and see what we can do with this new tool!"
      ],
      "metadata": {
        "id": "h6KblfsJ1Rf-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and Parse PDFs\n",
        "\n",
        "We'll start, as always, by grabbing some dependencies."
      ],
      "metadata": {
        "id": "X9FkgdSN2i1Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9K44OSqKhPrh",
        "outputId": "d353196d-be72-4233-f2b1-972f7cc4c004"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.0/284.0 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU llama-index llama-parse"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll need to provide a LlamaCloud API key to continue.\n",
        "\n",
        "You can find this by following these steps:\n",
        "\n",
        "1. Sign in with one of their many SSO options.\n",
        "  - ![image](https://i.imgur.com/WFH6CPK.png)\n",
        "2. Navigate to the References in the bottom left hand corner of the screen and select `API Key`.\n",
        "  - ![image](https://i.imgur.com/nlw1mo2.png)\n",
        "3. Generate a new key, name it, and keep it in a safe place!\n",
        "  - ![image](https://i.imgur.com/Rxshpeq.png)\n",
        "\n",
        "\n",
        "Now that we have our API Key - let's provide it as an environment variable below.\n",
        "\n",
        "You can also pass the key directly into the `LlamaParse` object we'll create later."
      ],
      "metadata": {
        "id": "8IxioOtR2pQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"LLAMA_CLOUD_API_KEY\"] = getpass.getpass(\"LLamaParse API Key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BuHZ38MhjzH",
        "outputId": "0785dd20-2c82-40c1-90b8-f9fc5b1b9675"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLamaParse API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we'll be using OpenAI as our LLM today - we'll need to pass that API key as well."
      ],
      "metadata": {
        "id": "VU4-fvh73UGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMQ_kwt-n5Lf",
        "outputId": "fd7461de-bfb1-47e3-aaf1-5b62061bb518"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make sure we can run async in our Colab instance."
      ],
      "metadata": {
        "id": "dQGJFiok3YBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "jAju5QV-iCNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LlamaParse Initialization\n",
        "\n",
        "Here we can initialize our `LlamaParse` object.\n",
        "\n",
        "Notice that there's a few parameters worth paying attention to:\n",
        "\n",
        "- `result_type` - at time of writing this notebook the options are limited to `\"text\"` and `\"markdown\"`. Markdown will be our choice as it will retain structured information quite nicely.\n",
        "- `num_workers` - this will let us set how many workers we'll need. Generally we'll want to set this to the number of files we're going to need to parse. (the maximum is `10`)"
      ],
      "metadata": {
        "id": "uiXGKNd43cXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_parse import LlamaParse\n",
        "\n",
        "parser = LlamaParse(\n",
        "    result_type=\"markdown\",\n",
        "    verbose=True,\n",
        "    language=\"en\",\n",
        "    num_workers=2,\n",
        ")"
      ],
      "metadata": {
        "id": "O6qrLAwpiN55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Uploading Files\n",
        "\n",
        "We'll next need to upload some files to test our the parser!\n",
        "\n",
        "Let's use [NVIDIA's 10-K](https://d18rn0p25nwr6d.cloudfront.net/CIK-0001045810/1cbe8fe7-e08a-46e3-8dcc-b429fc06c1a4.pdf) and the [Office of Educational Technology's AI and the Future of Learning report](https://www2.ed.gov/documents/ai-report/ai-report.pdf).\n",
        "\n",
        "You can upload them below - be careful to make sure the file matches with what you've uploaded!"
      ],
      "metadata": {
        "id": "9Klt9HvD4B1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "nvidia_earnings_report = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "riyF7R_ljPVN",
        "outputId": "4029871d-2c3c-4567-fb68-f70cf11642e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-74cd9750-4293-484d-ba59-364fa6f9c3d5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-74cd9750-4293-484d-ba59-364fa6f9c3d5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving nvidia-earnings.pdf to nvidia-earnings (1).pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ai_report = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "9mOEv1kelBIU",
        "outputId": "1fb80641-9279-4cf6-fc8c-d68ab37979db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5f12a551-cfe8-4ff2-bb5a-082422c4912b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5f12a551-cfe8-4ff2-bb5a-082422c4912b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving ai-report.pdf to ai-report (1).pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parsing Our Files\n",
        "\n",
        "Now that we've uploaded our files and set-up our `LlamaParser` we're ready to parse some files!\n",
        "\n",
        "Running this cell seems very inconsistent - with some files taking ~6min., and others taking ~4s. It seems there is some level of caching, but you can medium -> long wait times for this next cell.\n",
        "\n",
        "> NOTE: As of time of writing, only `.pdf` files are accepted."
      ],
      "metadata": {
        "id": "afMigjas4xve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = parser.load_data([\"./nvidia-earnings.pdf\", \"./ai-report.pdf\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkJuc1ypj8ar",
        "outputId": "8662b610-bcfd-4114-cf23-4383984175db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started parsing the file under job_id 559673ec-f538-4afc-bd2b-9ca13d8627f8\n",
            "Started parsing the file under job_id 0348c448-f308-44a0-aaa3-cc7055f9014e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at our 10-K example!"
      ],
      "metadata": {
        "id": "P3ZrdM5s5Jd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(documents[0].text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHR__3VinSEu",
        "outputId": "9642e716-8fe1-49e3-815c-160c50153511"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|Content|Page Number|\n",
            "|---|---|\n",
            "|Table of Contents| |\n",
            "|UNITED STATES SECURITIES AND EXCHANGE COMMISSION Washington, D.C. 20549| |\n",
            "|FORM 10-K| |\n",
            "|ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934 For the fiscal year ended January 28, 2024| |\n",
            "|Commission file number: 0-23985| |\n",
            "|NVIDIA NVIDIA CORPORATION (Exact name of registrant as specified in its charter)| |\n",
            "|Delaware 94-3177549| |\n",
            "|2788 San Tomas Expressway, Santa Clara, California 95051 (Address of principal executive offices)| |\n",
            "|Registrant’s telephone number, including area code: (408) 486-2000| |\n",
            "|Securities registered pursuant to Section 12(b) of the Act:| |\n",
            "|Title of each class|Trading Symbol(s)|Name of each exchange on which registered|\n",
            "|Common Stock, $0.001 par value per share|NVDA|The Nasdaq Global Select Market|\n",
            "|Securities registered pursuant to Section 12(g) of the Act:|None|\n",
            "|Indicate by check mark if the registrant is a well-known seasoned issuer, as defined in Rule 405 of the Securiti\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Right away we can see that some kind of structure is being retained!"
      ],
      "metadata": {
        "id": "k1BdkNPy5R37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(documents[1].text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jsw6GsznYxK",
        "outputId": "4d23177f-01ba-4513-cd13-e77b4a6a6e77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# OFFICE OF Artificial Intelligence Educational Technology and the Future of Teaching and Learning Insights and Recommendations May 2023\n",
            "---\n",
            "## Artificial Intelligence and the Future of Teaching and Learning\n",
            "\n",
            "Miguel A. Cardona, Ed.D.\n",
            "Secretary, U.S. Department of Education\n",
            "\n",
            "Roberto J. Rodríguez\n",
            "Assistant Secretary, Office of Planning, Evaluation, and Policy Development\n",
            "\n",
            "Kristina Ishmael\n",
            "Deputy Director, Office of Educational Technology\n",
            "\n",
            "May 2023\n",
            "\n",
            "Examples Are Not Endorsements\n",
            "\n",
            "This document contains examples and resource materials that are provided for the user’s convenience. The inclusion of any material is not intended to reflect its importance nor is it intended to endorse any views expressed or products or services offered. These materials may contain the views and recommendations of various subject matter experts as well as hypertext links, contact addresses, and websites to information created and maintained by other public and private organizations. The opinions expressed in any\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The same is true of our AI Education report!"
      ],
      "metadata": {
        "id": "s4OYuTZt5XTU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LlamaIndex Recursive Query Engine\n",
        "\n",
        "Now that we have some parsed objects - let's see how well we can leverage them using one of the [example query engines](https://github.com/run-llama/llama_parse/blob/main/examples/demo_advanced.ipynb)."
      ],
      "metadata": {
        "id": "WDv5moIApMfA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting our...Settings\n",
        "\n",
        "That's right! `ServiceContext` is dead, long live `Settings`.\n",
        "\n",
        "Let's point our generic LLM to `gpt-3.5-turbo` and our generic embedding model as `text-embedding-3-small`.\n",
        "\n",
        "> NOTE: You'll notice we're pulling `Settings` our of `llama_index.core` which is a major part of their `v0.10` update!"
      ],
      "metadata": {
        "id": "eee3nPrL5mmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Settings\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
      ],
      "metadata": {
        "id": "lRD5vth1pV_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to use a `MarkdownElementNodeParser` to help make sense of our Markdown objects so we can leverage the potentially structured information in the parsed documents.\n",
        "\n",
        "- Check out the [docs](https://docs.llamaindex.ai/en/stable/api/llama_index.core.node_parser.MarkdownElementNodeParser.html)"
      ],
      "metadata": {
        "id": "hShvdoLK6bmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.node_parser import MarkdownElementNodeParser\n",
        "\n",
        "node_parser = MarkdownElementNodeParser(llm=OpenAI(model=\"gpt-3.5-turbo\"), num_workers=8)"
      ],
      "metadata": {
        "id": "XK4YTyR-nZl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's parse!\n",
        "\n",
        "> NOTE: There appears to be inconsistent errors - but the parser is largely able to extract and understand structured data within the document provided by the parser"
      ],
      "metadata": {
        "id": "GMvADx2I65qa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nodes = node_parser.get_nodes_from_documents(documents=[documents[0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jdF-BnJoGzD",
        "outputId": "e83cfa86-b517-4032-e120-b8625a263b9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings have been explicitly disabled. Using MockEmbedding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "108it [00:00, 5435.12it/s]\n",
            " 42%|████▏     | 45/108 [00:16<00:17,  3.51it/s]WARNING:llama_index.core.response_synthesizers.refine:Validation error on structured response: 1 validation error for TableOutput\n",
            "columns\n",
            "  field required (type=value_error.missing)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 439, in _agive_response_single\n",
            "    structured_response = await program.acall(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 76, in acall\n",
            "    answer = await self._llm.astructured_predict(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/llms/llm.py\", line 235, in astructured_predict\n",
            "    return await program.acall(**prompt_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/program/openai/base.py\", line 220, in acall\n",
            "    return _parse_tool_calls(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/program/openai/base.py\", line 64, in _parse_tool_calls\n",
            "    output = output_cls.parse_raw(function_call.arguments)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\", line 549, in parse_raw\n",
            "    return cls.parse_obj(obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\", line 526, in parse_obj\n",
            "    return cls(**obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\", line 341, in __init__\n",
            "    raise validation_error\n",
            "pydantic.v1.error_wrappers.ValidationError: 1 validation error for TableOutput\n",
            "columns\n",
            "  field required (type=value_error.missing)\n",
            " 43%|████▎     | 46/108 [00:17<00:22,  2.72it/s]WARNING:llama_index.core.response_synthesizers.refine:Validation error on structured response: 1 validation error for TableOutput\n",
            "columns\n",
            "  field required (type=value_error.missing)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 439, in _agive_response_single\n",
            "    structured_response = await program.acall(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 76, in acall\n",
            "    answer = await self._llm.astructured_predict(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/llms/llm.py\", line 235, in astructured_predict\n",
            "    return await program.acall(**prompt_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/program/openai/base.py\", line 220, in acall\n",
            "    return _parse_tool_calls(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/program/openai/base.py\", line 64, in _parse_tool_calls\n",
            "    output = output_cls.parse_raw(function_call.arguments)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\", line 549, in parse_raw\n",
            "    return cls.parse_obj(obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\", line 526, in parse_obj\n",
            "    return cls(**obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\", line 341, in __init__\n",
            "    raise validation_error\n",
            "pydantic.v1.error_wrappers.ValidationError: 1 validation error for TableOutput\n",
            "columns\n",
            "  field required (type=value_error.missing)\n",
            "WARNING:llama_index.core.response_synthesizers.refine:Validation error on structured response: 1 validation error for TableOutput\n",
            "columns\n",
            "  field required (type=value_error.missing)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 439, in _agive_response_single\n",
            "    structured_response = await program.acall(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 76, in acall\n",
            "    answer = await self._llm.astructured_predict(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/llms/llm.py\", line 235, in astructured_predict\n",
            "    return await program.acall(**prompt_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/program/openai/base.py\", line 220, in acall\n",
            "    return _parse_tool_calls(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/program/openai/base.py\", line 64, in _parse_tool_calls\n",
            "    output = output_cls.parse_raw(function_call.arguments)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\", line 549, in parse_raw\n",
            "    return cls.parse_obj(obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\", line 526, in parse_obj\n",
            "    return cls(**obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\", line 341, in __init__\n",
            "    raise validation_error\n",
            "pydantic.v1.error_wrappers.ValidationError: 1 validation error for TableOutput\n",
            "columns\n",
            "  field required (type=value_error.missing)\n",
            " 51%|█████     | 55/108 [00:20<00:19,  2.70it/s]WARNING:llama_index.core.response_synthesizers.refine:Validation error on structured response: 1 validation error for TableOutput\n",
            "columns\n",
            "  field required (type=value_error.missing)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 439, in _agive_response_single\n",
            "    structured_response = await program.acall(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 76, in acall\n",
            "    answer = await self._llm.astructured_predict(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/llms/llm.py\", line 235, in astructured_predict\n",
            "    return await program.acall(**prompt_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/program/openai/base.py\", line 220, in acall\n",
            "    return _parse_tool_calls(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/program/openai/base.py\", line 64, in _parse_tool_calls\n",
            "    output = output_cls.parse_raw(function_call.arguments)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\", line 549, in parse_raw\n",
            "    return cls.parse_obj(obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\", line 526, in parse_obj\n",
            "    return cls(**obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\", line 341, in __init__\n",
            "    raise validation_error\n",
            "pydantic.v1.error_wrappers.ValidationError: 1 validation error for TableOutput\n",
            "columns\n",
            "  field required (type=value_error.missing)\n",
            " 54%|█████▎    | 58/108 [00:21<00:16,  3.02it/s]WARNING:llama_index.core.response_synthesizers.refine:Validation error on structured response: 1 validation error for TableOutput\n",
            "columns\n",
            "  field required (type=value_error.missing)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 439, in _agive_response_single\n",
            "    structured_response = await program.acall(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 76, in acall\n",
            "    answer = await self._llm.astructured_predict(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/llms/llm.py\", line 235, in astructured_predict\n",
            "    return await program.acall(**prompt_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/program/openai/base.py\", line 220, in acall\n",
            "    return _parse_tool_calls(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/program/openai/base.py\", line 64, in _parse_tool_calls\n",
            "    output = output_cls.parse_raw(function_call.arguments)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\", line 549, in parse_raw\n",
            "    return cls.parse_obj(obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\", line 526, in parse_obj\n",
            "    return cls(**obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\", line 341, in __init__\n",
            "    raise validation_error\n",
            "pydantic.v1.error_wrappers.ValidationError: 1 validation error for TableOutput\n",
            "columns\n",
            "  field required (type=value_error.missing)\n",
            " 55%|█████▍    | 59/108 [00:22<00:19,  2.52it/s]WARNING:llama_index.core.response_synthesizers.refine:Validation error on structured response: 1 validation error for TableOutput\n",
            "columns\n",
            "  field required (type=value_error.missing)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 439, in _agive_response_single\n",
            "    structured_response = await program.acall(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 76, in acall\n",
            "    answer = await self._llm.astructured_predict(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/llms/llm.py\", line 235, in astructured_predict\n",
            "    return await program.acall(**prompt_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/program/openai/base.py\", line 220, in acall\n",
            "    return _parse_tool_calls(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/program/openai/base.py\", line 64, in _parse_tool_calls\n",
            "    output = output_cls.parse_raw(function_call.arguments)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\", line 549, in parse_raw\n",
            "    return cls.parse_obj(obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\", line 526, in parse_obj\n",
            "    return cls(**obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\", line 341, in __init__\n",
            "    raise validation_error\n",
            "pydantic.v1.error_wrappers.ValidationError: 1 validation error for TableOutput\n",
            "columns\n",
            "  field required (type=value_error.missing)\n",
            " 77%|███████▋  | 83/108 [00:29<00:06,  3.73it/s]WARNING:llama_index.core.response_synthesizers.refine:Validation error on structured response: 1 validation error for TableOutput\n",
            "columns\n",
            "  field required (type=value_error.missing)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 439, in _agive_response_single\n",
            "    structured_response = await program.acall(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 76, in acall\n",
            "    answer = await self._llm.astructured_predict(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/llms/llm.py\", line 235, in astructured_predict\n",
            "    return await program.acall(**prompt_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/program/openai/base.py\", line 220, in acall\n",
            "    return _parse_tool_calls(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/program/openai/base.py\", line 64, in _parse_tool_calls\n",
            "    output = output_cls.parse_raw(function_call.arguments)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\", line 549, in parse_raw\n",
            "    return cls.parse_obj(obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\", line 526, in parse_obj\n",
            "    return cls(**obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\", line 341, in __init__\n",
            "    raise validation_error\n",
            "pydantic.v1.error_wrappers.ValidationError: 1 validation error for TableOutput\n",
            "columns\n",
            "  field required (type=value_error.missing)\n",
            " 89%|████████▉ | 96/108 [00:33<00:03,  3.51it/s]WARNING:llama_index.core.response_synthesizers.refine:Validation error on structured response: 1 validation error for TableOutput\n",
            "columns\n",
            "  field required (type=value_error.missing)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 439, in _agive_response_single\n",
            "    structured_response = await program.acall(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 76, in acall\n",
            "    answer = await self._llm.astructured_predict(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/llms/llm.py\", line 235, in astructured_predict\n",
            "    return await program.acall(**prompt_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/program/openai/base.py\", line 220, in acall\n",
            "    return _parse_tool_calls(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/program/openai/base.py\", line 64, in _parse_tool_calls\n",
            "    output = output_cls.parse_raw(function_call.arguments)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\", line 549, in parse_raw\n",
            "    return cls.parse_obj(obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\", line 526, in parse_obj\n",
            "    return cls(**obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\", line 341, in __init__\n",
            "    raise validation_error\n",
            "pydantic.v1.error_wrappers.ValidationError: 1 validation error for TableOutput\n",
            "columns\n",
            "  field required (type=value_error.missing)\n",
            " 90%|████████▉ | 97/108 [00:34<00:03,  3.21it/s]WARNING:llama_index.core.response_synthesizers.refine:Validation error on structured response: 1 validation error for TableOutput\n",
            "columns\n",
            "  field required (type=value_error.missing)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 439, in _agive_response_single\n",
            "    structured_response = await program.acall(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/response_synthesizers/refine.py\", line 76, in acall\n",
            "    answer = await self._llm.astructured_predict(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/core/llms/llm.py\", line 235, in astructured_predict\n",
            "    return await program.acall(**prompt_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/program/openai/base.py\", line 220, in acall\n",
            "    return _parse_tool_calls(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_index/program/openai/base.py\", line 64, in _parse_tool_calls\n",
            "    output = output_cls.parse_raw(function_call.arguments)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\", line 549, in parse_raw\n",
            "    return cls.parse_obj(obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\", line 526, in parse_obj\n",
            "    return cls(**obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\", line 341, in __init__\n",
            "    raise validation_error\n",
            "pydantic.v1.error_wrappers.ValidationError: 1 validation error for TableOutput\n",
            "columns\n",
            "  field required (type=value_error.missing)\n",
            "100%|██████████| 108/108 [00:55<00:00,  1.95it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can extract our `base_nodes` and `objects` to create our `VectorStoreIndex`."
      ],
      "metadata": {
        "id": "SZE-gV5d7F1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_nodes, objects = node_parser.get_nodes_and_objects(nodes)"
      ],
      "metadata": {
        "id": "b8IadyiYoM4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's build the index!"
      ],
      "metadata": {
        "id": "FhIYRTPH9H6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "recursive_index = VectorStoreIndex(nodes=base_nodes+objects)"
      ],
      "metadata": {
        "id": "9A7H17xlo7Vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recursive Query Engine\n",
        "\n",
        "Now we can build our Recursive Query Engine with reranking!\n",
        "\n",
        "We'll need to do a few steps:\n",
        "\n",
        "1. Initalize our reranker using `FlagEmbeddingReranker` powered by the `BAAI/bge-reranker-large`.\n",
        "2. Set up our recursive query engine!\n",
        "\n",
        "First, let's install some requirements."
      ],
      "metadata": {
        "id": "63zQ-SmM9nu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU llama-index-postprocessor-flag-embedding-reranker git+https://github.com/FlagOpen/FlagEmbedding.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGhgMxPyqOFS",
        "outputId": "46559688-daa1-4e9d-ffbd-0689f91b6db6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.3/156.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for FlagEmbedding (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First up, we'll initialize our reranker - we'll be leveraging [this](https://github.com/FlagOpen/FlagEmbedding) repo to leverage our [`BAAI/bge-reranker-large`](https://huggingface.co/BAAI/bge-reranker-large).\n",
        "\n",
        "Once that's done - we can follow a fairly standard flow of creating our query engine!"
      ],
      "metadata": {
        "id": "wEC7Er8o_NOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker\n",
        "\n",
        "reranker = FlagEmbeddingReranker(\n",
        "    top_n=5,\n",
        "    model=\"BAAI/bge-reranker-large\",\n",
        ")\n",
        "\n",
        "recursive_query_engine = recursive_index.as_query_engine(\n",
        "    similarity_top_k=15,\n",
        "    node_postprocessors=[reranker],\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLlEWwpFqFrp",
        "outputId": "76f586fe-8a37-42cc-9a0e-9545aa4ad07f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NVIDIA 10-K Test\n",
        "\n",
        "Now we can test this on our documents! Let's start with our 10-K document."
      ],
      "metadata": {
        "id": "W9uc7dxr_zx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Who is the E-VP, Operations - and how old are they?\"\n",
        "response = recursive_query_engine.query(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atjlGCjmq6y7",
        "outputId": "75b64110-49a3-49fb-910b-29014eb33051"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;2;11;159;203mRetrieval entering id_95ebb8c0-3296-49ef-af82-192aa1916fc1_42_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query Who is the E-VP, Operations - and how old are they?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_95ebb8c0-3296-49ef-af82-192aa1916fc1_44_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query Who is the E-VP, Operations - and how old are they?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_95ebb8c0-3296-49ef-af82-192aa1916fc1_270_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query Who is the E-VP, Operations - and how old are they?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_95ebb8c0-3296-49ef-af82-192aa1916fc1_190_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query Who is the E-VP, Operations - and how old are they?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_95ebb8c0-3296-49ef-af82-192aa1916fc1_14_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query Who is the E-VP, Operations - and how old are they?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_95ebb8c0-3296-49ef-af82-192aa1916fc1_48_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query Who is the E-VP, Operations - and how old are they?\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5R6XBRbTrIeR",
        "outputId": "5b669f4c-e58d-4952-d90e-81093b63d845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debora Shoquist is the Executive Vice President of Operations, and she is 69 years old.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://i.imgur.com/OZcPlJw.png)"
      ],
      "metadata": {
        "id": "n_AYaHdSw-kf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see - this information was retrieved extremely well!\n",
        "\n",
        "> NOTE: The actual response time was in the 2-3min. timeframe for the full query which is likely due to running this instance on CPU - meaning the reranking process was a bottleneck. You may find better performance running this notebook in a GPU enabled instance."
      ],
      "metadata": {
        "id": "795ao5uWAIfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the gross carrying amount of Total Amortizable Intangible Assets for Jan 29, 2023?\"\n",
        "response = recursive_query_engine.query(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeJZq58ns6j9",
        "outputId": "e662b834-5b9f-4a06-df7a-ce13a78450dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;2;11;159;203mRetrieval entering id_95ebb8c0-3296-49ef-af82-192aa1916fc1_198_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query What is the gross carrying amount of Total Amortizable Intangible Assets for Jan 29, 2023?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_95ebb8c0-3296-49ef-af82-192aa1916fc1_214_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query What is the gross carrying amount of Total Amortizable Intangible Assets for Jan 29, 2023?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_95ebb8c0-3296-49ef-af82-192aa1916fc1_256_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query What is the gross carrying amount of Total Amortizable Intangible Assets for Jan 29, 2023?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_95ebb8c0-3296-49ef-af82-192aa1916fc1_228_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query What is the gross carrying amount of Total Amortizable Intangible Assets for Jan 29, 2023?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_95ebb8c0-3296-49ef-af82-192aa1916fc1_250_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query What is the gross carrying amount of Total Amortizable Intangible Assets for Jan 29, 2023?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_95ebb8c0-3296-49ef-af82-192aa1916fc1_222_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query What is the gross carrying amount of Total Amortizable Intangible Assets for Jan 29, 2023?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_95ebb8c0-3296-49ef-af82-192aa1916fc1_238_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query What is the gross carrying amount of Total Amortizable Intangible Assets for Jan 29, 2023?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_95ebb8c0-3296-49ef-af82-192aa1916fc1_130_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query What is the gross carrying amount of Total Amortizable Intangible Assets for Jan 29, 2023?\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xu4KqhmbtIZ5",
        "outputId": "22f54064-e384-4112-d835-2eb4a09c27fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The gross carrying amount of Total Amortizable Intangible Assets for Jan 29, 2023 is $3,539 million.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://i.imgur.com/9jwFpWk.png)"
      ],
      "metadata": {
        "id": "Mp3GBAQdumLk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another big win for LlamaParse!"
      ],
      "metadata": {
        "id": "_z2vazxzAeyh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing it on the AI Education Report\n",
        "\n",
        "The results for the 10-K were incredible - but will the AI Education Report hold up?"
      ],
      "metadata": {
        "id": "0nRyIAwzAvZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ai_report_nodes = node_parser.get_nodes_from_documents(documents=[documents[1]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N30vLMMmunNx",
        "outputId": "4593d7af-0e86-4a94-fd2f-c5784cf73db1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings have been explicitly disabled. Using MockEmbedding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "19it [00:00, 6814.76it/s]\n",
            "100%|██████████| 19/19 [00:13<00:00,  1.42it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ai_base_nodes, ai_objects = node_parser.get_nodes_and_objects(ai_report_nodes)"
      ],
      "metadata": {
        "id": "T-sy_bQ8vsgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ai_recursive_index = VectorStoreIndex(nodes=ai_base_nodes+ai_objects)"
      ],
      "metadata": {
        "id": "Ke8_eEvhvrU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reranker = FlagEmbeddingReranker(\n",
        "    top_n=5,\n",
        "    model=\"BAAI/bge-reranker-large\",\n",
        ")\n",
        "\n",
        "ai_recursive_query_engine = ai_recursive_index.as_query_engine(\n",
        "    similarity_top_k=15,\n",
        "    node_postprocessors=[reranker],\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "yGZFBdMUv2g5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How many AI publications on pattern recognition was there in 2020?\"\n",
        "response = ai_recursive_query_engine.query(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQYUVBLkwAu7",
        "outputId": "4f3b9132-8ad1-4b3d-e94a-40c2e0ad7b71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;2;11;159;203mRetrieval entering id_b7e493fe-7605-4671-ad3c-c676cc460d63_12_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query How many AI publications on pattern recognition was there in 2020?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_b7e493fe-7605-4671-ad3c-c676cc460d63_78_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query How many AI publications on pattern recognition was there in 2020?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_b7e493fe-7605-4671-ad3c-c676cc460d63_4_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query How many AI publications on pattern recognition was there in 2020?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_b7e493fe-7605-4671-ad3c-c676cc460d63_28_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query How many AI publications on pattern recognition was there in 2020?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_b7e493fe-7605-4671-ad3c-c676cc460d63_60_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query How many AI publications on pattern recognition was there in 2020?\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7PyZFGHwa4P",
        "outputId": "725eceee-7737-46a1-fc5a-f785230a1ea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There were 30.07 AI publications on pattern recognition in 2020.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://i.imgur.com/tbGtUX2.png)"
      ],
      "metadata": {
        "id": "v_FIOOyBxI3d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the query engine *did* retrieve context that was literally on the figure - it was not the correct information, in any way."
      ],
      "metadata": {
        "id": "6EIwtcJcA5mj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Can you describe what Figure 14 is related to?\"\n",
        "response = ai_recursive_query_engine.query(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5C_f3OfRwR3O",
        "outputId": "1746b1e9-625f-412e-cda9-4dcada6bde72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;2;11;159;203mRetrieval entering id_b7e493fe-7605-4671-ad3c-c676cc460d63_52_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query Can you describe what Figure 14 is related to?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_b7e493fe-7605-4671-ad3c-c676cc460d63_28_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query Can you describe what Figure 14 is related to?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_b7e493fe-7605-4671-ad3c-c676cc460d63_100_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query Can you describe what Figure 14 is related to?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_b7e493fe-7605-4671-ad3c-c676cc460d63_56_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query Can you describe what Figure 14 is related to?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_b7e493fe-7605-4671-ad3c-c676cc460d63_38_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query Can you describe what Figure 14 is related to?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_b7e493fe-7605-4671-ad3c-c676cc460d63_4_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query Can you describe what Figure 14 is related to?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_b7e493fe-7605-4671-ad3c-c676cc460d63_42_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query Can you describe what Figure 14 is related to?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_b7e493fe-7605-4671-ad3c-c676cc460d63_60_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query Can you describe what Figure 14 is related to?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_b7e493fe-7605-4671-ad3c-c676cc460d63_58_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query Can you describe what Figure 14 is related to?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_b7e493fe-7605-4671-ad3c-c676cc460d63_22_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query Can you describe what Figure 14 is related to?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_b7e493fe-7605-4671-ad3c-c676cc460d63_66_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query Can you describe what Figure 14 is related to?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering id_b7e493fe-7605-4671-ad3c-c676cc460d63_112_table: TextNode\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query Can you describe what Figure 14 is related to?\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsgT_ohHxHme",
        "outputId": "194a0dde-8f4b-40a5-e642-a2cfe3da8170"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Figure 14 is related to the long tail of learner variability in the context of AI in education. It illustrates how learners vary in their strengths and needs, emphasizing the importance of addressing a wider spectrum of strengths and needs beyond just the most typical cases. The figure highlights the potential of AI to cater to a diverse range of learners by focusing on the long tail of learner variability rather than solely targeting the most common learning profiles.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://i.imgur.com/T7nVQj8.png)"
      ],
      "metadata": {
        "id": "GuVC2-YnysWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see - the query engine did not successfully retrieve context related to the correct Figure. If you read the report, you'll notice that it found information related to Fig. 13."
      ],
      "metadata": {
        "id": "f7l5TZFuBB7j"
      }
    }
  ]
}